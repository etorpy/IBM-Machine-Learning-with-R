{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.bigdatauniversity.com\"><img src = \"https://ibm.box.com/shared/static/cw2c7r3o20w9zn8gkecaeyjhgw3xdgbj.png\" width = 400, align = \"center\"></a>\n",
    "\n",
    "<h1 align=center><font size = 5> Principal Component Analysis (PCA) in R </font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "In this exercise, we will be using the <font color = \"green\">credit_Fraud.csv</font> data set. This dataset contains various information on the credit history of customers of a financial institution.\n",
    "\n",
    "The objective of this exercise is to use principal component analysis to reduce the number of features of the dataset into a smaller set of linearly uncorrelated features (principal components)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data <- read.csv(\"https://ibm.box.com/shared/static/78g5lc5krkvclwbnxi6zodemlktr9v9r.csv\", header=T)\n",
    "head(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data description\n",
    "Data is provided in its original form using .arff file which is a complete package with meta-data of the data in it; In this data there are 7 numerical variable: **credit_usage, current_balance, cc_age, residence_since, num_dependence, existing_credit, location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of column index that are numeric\n",
    "numvec=c();\n",
    "for( i in 1:ncol(data)){\n",
    "    if ( is.numeric(data[,i])) numvec = append(numvec,i)\n",
    "}\n",
    "cat('Numeric columns # ', numvec, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let subset the numeric variable and plots the box plots to see their distribution approximately\n",
    "datn = data[,numvec]\n",
    "\n",
    "head(datn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question 1**\n",
    "\n",
    "Do we need to standardize ( location and scale) ? or do we need any kind of transformation before doing location/scale standardization ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your Answer Code Here ## \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">\n",
    "<a href=\"#p1\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for the solution</a>\n",
    "</div>\n",
    "<div id=\"p1\" class=\"collapse\">\n",
    "```\n",
    "# NOTE: that the distribution of variables are pretty skewed and transformation is needed in order to\n",
    "#       make them comparable to plot, so instead of transforming each variable we transform the y-axis here\n",
    "boxplot(datn,col=\"blue\",pch=20, ylab=\"on original scale\")\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "As you can see from the plot above, one of the variables **current_balance** has larger values and a wider spread compared to the other variables so scaling would be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "How can we reduce these 7 features into a smaller number of features, such that at least 60 to 70% of the variability is explained by these newly obtained features; thus reducing the size of the data, also known as dimensionality reduction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your Answer Code Here ## \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">\n",
    "<a href=\"#p2\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for the solution</a>\n",
    "</div>\n",
    "<div id=\"p2\" class=\"collapse\">\n",
    "```\n",
    "# Perform a principal component analysis on the data and print the results\n",
    "pcasolutions = prcomp(datn, center=TRUE, scale=TRUE)\n",
    "\n",
    "print(pcasolutions)\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " ## check  you answers:\n",
    "options(warn=-1)\n",
    "if ( length(pcasolutions$scale)==1 & !(pcasolutions$scale)) { \n",
    "    cat(\" Scaling of variables is necessary here, ... did you make use of \\\"scale=TRUE\\\" option ? \")\n",
    "}else{\n",
    "    cat(\" passed ....\")\n",
    "}\n",
    "\n",
    "options(warn=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a scree plot and apply the Kaiser-Gutmann cutoff rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your Answer Code Here ## \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">\n",
    "<a href=\"#p3\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for the solution</a>\n",
    "</div>\n",
    "<div id=\"p3\" class=\"collapse\">\n",
    "```\n",
    "#eigen-values : \n",
    "eigenvalues = pcasolutions$sdev**2\n",
    "\n",
    "# cumulative proportion of variance explained by each principal component:\n",
    "prop = cumsum(eigenvalues)/sum(eigenvalues)\n",
    "\n",
    "plot(prop, type=\"b\", ylab=\"proportion of variance explained\", xlab=\"# of principal components\")\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Answer 2**\n",
    "\n",
    "The first three principal components explain 60% of the variablity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3. Complete the following Tasks:\n",
    "##### a.  Look at the scree plot and compare it with the proportion of variance explained plot to see what % of variance is explained. Let \"k\" be the number of principal components you decide to take\n",
    "##### b.  Plot the principal components and check how they align with the variable $class$  (indicating credit score) is spread around by colouring them green for $good$ and blue for $bad$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your Answer Code Here ## \n",
    "# a. #\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">\n",
    "<a href=\"#p4\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for the solution</a>\n",
    "</div>\n",
    "<div id=\"p4\" class=\"collapse\">\n",
    "```\n",
    "\n",
    "# taking a look at the first three PCs \n",
    "rotated_data = pcasolutions$x[,1:3]\n",
    "head(rotated_data)\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we want to look at the weights, aka factor loadings, associated with each of the original dimensions for our selected # of principal components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Your Answer Code Here ## \n",
    "# b. # \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"right\">\n",
    "<a href=\"#p5\" class=\"btn btn-default\" data-toggle=\"collapse\">Click here for the solution</a>\n",
    "</div>\n",
    "<div id=\"p5\" class=\"collapse\">\n",
    "```\n",
    "# Rotation gives you the factor loadings # \n",
    "eigenvectors = pcasolutions$rotation[,1:3]\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "options(warn=-1)\n",
    "if ( all(dim(eigenvectors) == c(ncol(datn),3) )){\n",
    "    if ( round(eigenvectors[1,1],4) == 0.6574 & round(eigenvectors[2,1],4) ==0.7180 )\n",
    "    cat(\"Passed !!\\n\")\n",
    "    \n",
    "}else{\n",
    "    cat(\"NOTE: perhaps you want to go back and check the creation of the PCAs? \\n\")\n",
    "}\n",
    "\n",
    "options(warn=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot...\n",
    "Let's plot the transformed data in 2D and 3D to see if we can find segregation of the data.\n",
    "Also, let's look at the correlation between the principal components obtained using numeric data and the categorical variables we had in our data.\n",
    "Note that there are 14 categorical variable you can choose to replace **catcol** in the code below by each of the variables to see if you see any segregation of the colours. If the categorical column is correlated to any of PC then you would see some kind of segregation depending on the magnitude of correlation.\n",
    "Also, feel free to change column name below for **catcol** and see the different plots generated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some colouring based on other categorical variables;\n",
    "catcol = 'class'\n",
    "colcolumn  = data[,catcol]\n",
    "l1 = levels(colcolumn)\n",
    "colpalatte = c('green','blue', 'cyan')\n",
    "mycol =colpalatte[as.numeric(colcolumn)]\n",
    "head(mycol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "install.packages(\"plot3D\")\n",
    "library(plot3D)\n",
    "\n",
    "plot(rotated_data[,1],rotated_data[,2],pch=20, col=mycol, xlab=\"First PC\", ylab=\"Second PC\",\n",
    "    main=paste('Plot of First 2 PC (colour by :',catcol, \")\"))\n",
    "legend(3.6,4, legend =l1, pch=20, col=colpalatte[1:length(l1)])\n",
    "\n",
    "plot(rotated_data[,2],rotated_data[,3],pch=20, col=mycol, xlab=\"Second PC\", ylab=\"Third PC\",\n",
    "    main=paste('Plot of 2nd and 3rd PC (colour by :',catcol, \")\"))\n",
    "legend(3,3.5, legend =l1, pch=20, col=colpalatte[1:length(l1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot3D::scatter3D(rotated_data[,1],rotated_data[,2],rotated_data[,3] , pch=20, bty='g',phi=0,cex=2,ticktype='detailed',\n",
    "                 colkey=FALSE, col = mycol, xlab=\"First PC\", ylab=\"Second PC\", zlab=\"Third PC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 3**\n",
    "The spread of the class is fairly intertwined indicating that principal components don't segregate the response. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to learn more?\n",
    "\n",
    "IBM SPSS Modeler is a comprehensive analytics platform that has many machine learning algorithms. It has been designed to bring predictive intelligence to decisions made by individuals, by groups, by systems – by your enterprise as a whole. A free trial is available through this course, available here: [SPSS Modeler for Mac users](https://cocl.us/ML0151EN_SPSSMod_mac) and [SPSS Modeler for Windows users](https://cocl.us/ML0151EN_SPSSMod_win)\n",
    "\n",
    "Also, you can use Data Science Experience to run these notebooks faster with bigger datasets. Data Science Experience is IBM's leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, DSX enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of DSX users today with a free account at [Data Science Experience](https://cocl.us/ML0151EN_DSX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this exercise!\n",
    "\n",
    "Notebook created by: Jag Rangrej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Copyright &copy; 2017 [IBM Cognitive Class](https://cocl.us/ML0151EN_cclab_cc). This notebook and its source code are released under the terms of the [MIT License](https://cognitiveclass.ai/mit-license/)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
