{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.bigdatauniversity.com\"><img src = \"https://ibm.box.com/shared/static/cw2c7r3o20w9zn8gkecaeyjhgw3xdgbj.png\" width = 400, align = \"center\"></a>\n",
    "\n",
    "\n",
    "<h1 align=center><font size = 5>PRINCIPAL COMPONENT ANALYSIS IN R</font></h1>\n",
    "\n",
    "---\n",
    "\n",
    "## What is Principal Component Analysis?\n",
    "Imagine you want to create a model to analyse and predict data. You have the structure for your model ready on your mind and are ready to create it. You get your dataset and upload it...\n",
    "\n",
    "<center>\n",
    "<img src=https://ibm.box.com/shared/static/buwt9pngja01j97gdcyu6gnzkz3rw9s4.jpg/>\n",
    "<font size=\"2\">*A dataset with a humongous amount of columns and rows.*</font>\n",
    "</center>\n",
    "\n",
    "...only to find that it is pretty big with many variables. With the increase in the number of dimensions, curse-of-dimensionality seeps in making it difficult for the estimation/optimization algorithm to converge. Many of these variables are highly correlated giving rise to collinearity issues in the simple regression based models vis-a-vis affecting model fit. \n",
    "\n",
    "Feature selection is process by which one can reduce the number of feature(s) either manually or automatically by eliminating or combining them, deleting some of the variables is the crudest form of reducing the dimensionality which almost alway leads to loss of information. It would be better if there was a way to condense the dimensions/features retaining optimal information.\n",
    "\n",
    "Principal Component Analysis (PCA) is a **dimensionality reduction process** that can be applied to reduce the amount of features (or dimensions) from a data set while explaining maximum amount of variance in the data. PCA is based on **eigenvector(s) and eigenvalue(s)**, which, to keep it simple are mathematical constructs representing directional-component and magnitude of the variance of the transformed feature(s) respectively. For each dimension, there is one eigenvector and eigenvalue pair that describes that dimension. We want to find the **components that have the largest variance**, that is, the eigenvectors that have the largest eigenvalues associated to them. These eigenvectors are mutually orthogonal, otherwise known as non-correlated or independent.\n",
    "\n",
    "Once we have the highest-variance components, we can represent our data using those instead of the original features and drop the ones with the lowest variances. This makes it so that we have few highly descriptive columns with the largest amounts of variance to differentiate our data points.\n",
    "\n",
    "---\n",
    "\n",
    "## Principal Component Analysis in R\n",
    "Now that you know what Principal Component Analysis is and what are its uses, we can move on to actually using it in R. For this, we are going to utilize the `iris` dataset, so we do not need to load any datasets into R.\n",
    "\n",
    "In R, there are a few different functions for analyzing Principal Components -- the preferred and most utilized one is `prcomp`.  `prcomp` takes the dataset you want to analyze as a parameter with two optional parameters (`center` and `scale.`), which do the work of centering (shifting your data points to be zero-centered) and scaling (so your observations have unit variance) your data, **which is recommended for PCA**.\n",
    "\n",
    "Note that the `iris` dataset has a `Species` column, which is non-numerical. For PCA to work, we need to remove that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve the \"iris\" dataset, sans the \"Species\" column.\n",
    "iris_numerical <- iris[,-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"scatterplot3d\") # Install\n",
    "library(\"scatterplot3d\") # load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatterplot3d(iris[,1:3],\n",
    "              main=\"3D Scatter Plot\",\n",
    "              xlab = \"Sepal Length (cm)\",\n",
    "              ylab = \"Sepal Width (cm)\",\n",
    "              zlab = \"Petal Length (cm)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the numerical data from \"iris\"\n",
    "# prcomp does all the work of centering and scaling our data for us!\n",
    "pcaSolution <- prcomp(iris_numerical, center=TRUE, scale.=TRUE)\n",
    "\n",
    "# Print a summary of the analysis\n",
    "print(pcaSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have our analysis done -- with the standard deviation and rotation (the eigenvectors!) printed out. However, we can extract even more information from our analysis by using `summary`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print a summary of the results\n",
    "summary(pcaSolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, **the first two principal components are very descriptive of our data** -- together they amount for more than **95% of the variance present in our data** (look at the cumulative proportion row). This is a pretty good lead that we should use them as our dimensions.\n",
    "\n",
    "However, there is another way of choosing what components are going to be used -- **Scree plots and the Kaiser-Guttman rule**. Scree plots are simple -- you just plot the eigenvalues for each component. It should be visible where there is a dropoff, which should be the cutoff for dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(pcaSolution, type=\"l\", main=\"Eigenvalues for each Principal Component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The Kaiser-Guttman rule states that one should pick the dimensions based on the average eigenvalue -- if a component presents a higher-than-average eigenvalue, then it should be picked. We can add this line to the Scree plot to visualize this cutoff better. Eigenvalues can be calculated as the square value of the standard deviation for a principal component, like this:\n",
    "\n",
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "Note: Considering that your data has been centered and scaled, the mean of the eigenvalues should be **equal to one**.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print the eigenvalues for the principal components.\n",
    "# The eigenvalues are calculated by squaring the standard deviation values for each component.\n",
    "eigenvalues <- (pcaSolution$sdev)**2\n",
    "eigenvalues\n",
    "\n",
    "{\n",
    "# Plot the Scree plot.\n",
    "screePlot <- plot(pcaSolution, type=\"l\", main=\"Eigenvalues with Kaiser-Guttman cutoff\")\n",
    "\n",
    "# Add a cutoff line based on the mean of the eigenvalues.\n",
    "# This should be equal to one for centered and scaled data.\n",
    "abline(h=mean(eigenvalues),lty=2,col=\"red\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case it appears that picking the first component, and *possibly* the second component seems like the correct choice. To get the actual data with the rotation applied (aligned to the component axes), you should retrieve the `x` column from our solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the values of the observations for each principal component.\n",
    "rotated_values <- pcaSolution$x\n",
    "\n",
    "# Print out the first six rows of data.\n",
    "head(rotated_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the two components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(rotated_values)\n",
    "rotated_values = as.data.frame(rotated_values)\n",
    "colors = iris[,5];\n",
    "levels(colors)= c(1,2,3)\n",
    "plot(rotated_values$PC1, rotated_values$PC2, xlab=\"Principal component 1\",\n",
    "    ylab=\"principal component 2\", pch = 20, cex=2,\n",
    "     col=adjustcolor(colors,alpha.f=0.5))\n",
    " legend(2,2, legend=levels(iris[,5]),pch=20,cex=0.8,col=levels(colors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This is the end of the \"Principal Component Analysis with R\" notebook. Hopefully, now you understand what Principal Component Analysis is, what it's used for and how to use it in an R environment. Thank you for reading this notebook, and good luck on your studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to learn more?\n",
    "\n",
    "IBM SPSS Modeler is a comprehensive analytics platform that has many machine learning algorithms. It has been designed to bring predictive intelligence to decisions made by individuals, by groups, by systems â€“ by your enterprise as a whole. A free trial is available through this course, available here: [SPSS Modeler for Mac users](https://cocl.us/ML0151EN_SPSSMod_mac) and [SPSS Modeler for Windows users](https://cocl.us/ML0151EN_SPSSMod_win)\n",
    "\n",
    "Also, you can use Data Science Experience to run these notebooks faster with bigger datasets. Data Science Experience is IBM's leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, DSX enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of DSX users today with a free account at [Data Science Experience](https://cocl.us/ML0151EN_DSX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thanks for completing this lesson!\n",
    "\n",
    "Notebook created by: <a href=\"https://br.linkedin.com/in/walter-gomes-de-amorim-junior-624726121\">Walter Gomes de Amorim Junior</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Copyright &copy; 2016 [Cognitive Class](https://cognitiveClass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
